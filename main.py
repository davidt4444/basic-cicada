import xml.etree.ElementTree as ET

#stuff needed to scrape rss feeds generated by javascript
import time 
import pandas as pd 
from selenium import webdriver 
from selenium.webdriver import Chrome 
from selenium.webdriver.common.by import By 
import html

def scrapeFeedData(url:str):
    # Define the Chrome webdriver options
    options = webdriver.ChromeOptions() 
    options.add_argument("--headless") # Set the Chrome webdriver to run in headless mode for scalability
    options.add_argument("--enable-javascript") # enable javascript for this to work
    #make sure to put Cicada/1.0 at the beginning of the User-Agent
    user_agent = 'Cicada/1.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36'
    options.add_argument(f'user-agent={user_agent}')


    # By default, Selenium waits for all resources to download before taking actions.
    # However, we don't need it as the page is populated with dynamically generated JavaScript code.
    options.page_load_strategy = "none"

    # Pass the defined options objects to initialize the web driver 
    driver = Chrome(options=options) 
    # Set an implicit wait of 5 seconds to allow time for elements to appear before throwing an exception
    driver.implicitly_wait(5)

    # wait 20 seconds to let the driver load the website completely.
    driver.get(url) 
    time.sleep(20)
    stuff = 'pre'
    if stuff in driver.page_source:
        rss = driver.find_element(By.TAG_NAME, stuff)
        encoded = rss.get_attribute("innerHTML")
        root = html.unescape(encoded)
        return parseFeed(root)
    return None

def parseFeed(feed:str):
    elements = []
    tree = ET.ElementTree(ET.fromstring(feed))
    root = tree.getroot()
    print(feed)
    # todo: put your xml parsing logic here

def main():
    scrapeFeedData('https://www.thenameofyourbrand.com/rss.html')

if __name__ == "__main__":
    main()
